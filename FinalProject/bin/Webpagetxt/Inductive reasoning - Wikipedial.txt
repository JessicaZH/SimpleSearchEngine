Inductive reasoning - Wikipedia Inductive reasoning From Wikipedia, the free encyclopedia Jump to navigation Jump to search "Inductive inference" redirects here. For the technique in mathematical proof, see Mathematical induction. This article's lead section may not adequately summarize its contents. Relevant discussion may be found on the talk page. To comply with Wikipedia's lead section guidelines, please consider modifying the lead to provide an accessible overview of the article's key points in such a way that it can stand on its own as a concise version of the article. (September 2018) This article may require copy editing for redundancies throughout. You can assist by editing it. (October 2018) (Learn how and when to remove this template message) Inductive reasoning is a method of reasoning in which the premises are viewed as supplying some evidence for the truth of the conclusion (in contrast to deductive reasoning and abductive reasoning). While the conclusion of a deductive argument is certain, the truth of the conclusion of an inductive argument may be probable, based upon the evidence given.[1] Many dictionaries define inductive reasoning as the derivation of general principles from specific observations, though some sources find this usage "outdated".[2] Contents 1 Comparison with deductive reasoning 2 History 2.1 Ancient philosophy 2.2 Early modern philosophy 2.3 Late modern philosophy 2.4 Contemporary philosophy 2.4.1 Bertrand Russell 2.4.2 Gilbert Harman 3 Criticism 3.1 Biases 4 Types and examples 4.1 Generalization 4.1.1 Statistical and inductive generalization 4.2 Statistical syllogism 4.3 Simple induction 4.4 Enumerative induction 4.5 Argument from analogy 4.6 Causal inference 4.7 Prediction 5 Bayesian inference 6 Inductive inference 7 See also 8 References 9 Further reading 10 External links Comparison with deductive reasoning[edit] Argument terminology Unlike deductive arguments, inductive reasoning allows for the possibility that the conclusion is false, even if all of the premises are true.[3] Instead of being valid or invalid, inductive arguments are either strong or weak, which describes how probable it is that the conclusion is true.[4] Another crucial difference is that deductive certainty is impossible in non-axiomatic systems, such as reality, leaving inductive reasoning as the primary route to (probabilistic) knowledge of such systems.[5] Given that "if A is true then that would cause B, C, and D to be true", an example of deduction would be "A is true therefore we can deduce that B, C, and D are true". An example of induction would be "B, C, and D are observed to be true therefore A might be true". A is a reasonable explanation for B, C, and D being true. For example: A large enough asteroid impact would create a very large crater and cause a severe impact winter that could drive the non-avian dinosaurs to extinction. We observe that there is a very large crater in the Gulf of Mexico dating to very near the time of the extinction of the non-avian dinosaurs Therefore it is possible that this impact could explain why the non-avian dinosaurs became extinct. Note however that this is not necessarily the case. Other events with the potential to affect global climate also coincide with the extinction of the non-avian dinosaurs. For example, the release of volcanic gases (particularly sulfur dioxide) during the formation of the Deccan Traps in India. A classical example of an incorrect inductive argument was presented by John Vickers: All of the swans we have seen are white. Therefore, we know that all swans are white. The correct conclusion would be, "We expect that all swans are white". The definition of inductive reasoning described in this article excludes mathematical induction, which is a form of deductive reasoning that is used to strictly prove properties of recursively defined sets.[6] The deductive nature of mathematical induction is based on the non-finite number of cases involved when using mathematical induction, in contrast with the finite number of cases involved in an enumerative induction procedure with a finite number of cases like proof by exhaustion. Both mathematical induction and proof by exhaustion are examples of complete induction. Complete induction is a type of masked deductive reasoning. An argument is deductive when the conclusion is necessary given the premises. That is, the conclusion cannot be false if the premises are true. If a deductive conclusion follows duly from its premises then it is valid; otherwise it is invalid (that an argument is invalid is not to say it is false. It may have a true conclusion, just not on account of the premises). An examination of the above examples will show that the relationship between premises and conclusion is such that the truth of the conclusion is already implicit in the premises. Bachelors are unmarried because we say they are; we have defined them so. Socrates is mortal because we have included him in a set of beings that are mortal. For inductive reasoning the premises or prior data provide support for the conclusion, but they do not guarantee it. The result is a conclusion having, it is often said, a “degree of certainty.” The phrase is not optimal since certainty is absolute and does not come in degrees; what is really meant is degrees approaching certainty. Succinctly put: deduction is about certainty/necessity; induction is about probability.[7] This is the best way to understand and remember the difference between inductive vs. deductive reasoning. Any single assertion will answer to one of these two criteria. (There is also modal logic, which deals with the distinction between the necessary and the possible in a way not concerned with probabilities among things deemed possible.) The philosophical definition of inductive reasoning is more nuanced than simple progression from particular/individual instances to broader generalizations. Rather, the premises of an inductive logical argument indicate some degree of support (inductive probability) for the conclusion but do not entail it; that is, they suggest truth but do not ensure it. In this manner, there is the possibility of moving from general statements to individual instances (for example, statistical syllogisms, discussed below). History[edit] Ancient philosophy[edit] For a move from particular to universal, Aristotle in the 300s BCE used the Greek word epagogé, which Cicero translated into the Latin word inductio.[8] In the 300s CE, Sextus Empiricus maintained that all knowledge derives from sensory experience—concluded in his Outlines of Pyrrhonism that acceptance of universal statements as true cannot be justified by induction.[8] Early modern philosophy[edit] In 1620, early modern philosopher Francis Bacon repudiated mere experience and enumerative induction, and sought to couple those with neutral and minute and many varied observations before[further explanation needed] to uncover the natural world's structure and causal relations beyond the present scope of experience via his method of inductivism, which nonetheless required enumerative induction as a component. The supposedly radical empiricist David Hume's 1740 stance found enumerative induction to have no rational, let alone logical, basis but to be a custom of the mind and an everyday requirement to live, although observations could be coupled with the principle uniformity of nature—another logically invalid conclusion, thus the problem of induction—to seemingly justify enumerative induction and reason toward unobservables, including causality counterfactually, simply that[further explanation needed] modifying such an aspect prevents or produces such outcome. Awakened from "dogmatic slumber" by a German translation of Hume's work, Kant sought to explain the possibility of metaphysics. In 1781, Kant's Critique of Pure Reason introduced the distinction rationalism, a path toward knowledge distinct from empiricism. Kant sorted statements into two types. The analytic are true by virtue of their terms' arrangement and meanings—thus are tautologies, merely logical truths, true by necessity—whereas the synthetic arrange meanings to refer to states of facts, contingencies. Finding it impossible to know objects as they truly are in themselves, however, Kant found the philosopher's task not peering behind the veil of appearance to view the noumena, but simply handling phenomena. Reasoning that the mind must contain its own categories organizing sense data, making experience of space and time possible, Kant concluded uniformity of nature a priori.[9] A class of synthetic statements was not contingent but true by necessity, then, the synthetic a priori. Kant thus saved both metaphysics and Newton's law of universal gravitation, but incidentally discarded scientific realism and developed transcendental idealism. Kant's transcendental idealism prompted the trend German idealism. G F W Hegel's absolute idealism flourished across continental Europe and fueled nationalism. Late modern philosophy[edit] Developed by Saint-Simon, and promulgated in the 1830s by his former student Comte was positivism, the first late modern philosophy of science. In the French Revolution's aftermath, fearing society's ruin again, Comte opposed metaphysics. Human knowledge had evolved from religion to metaphysics to science, said Comte, which had flowed from mathematics to astronomy to physics to chemistry to biology to sociology—in that order—describing increasingly intricate domains, all of society's knowledge having become scientific, as questions of theology and of metaphysics were unanswerable. Comte found enumerative induction reliable by its grounding on experience available, and asserted science's use as improving human society, not metaphysical truth. According to Comte, scientific method frames predictions, confirms them, and states laws—positive statements—irrefutable by theology or by metaphysics. Regarding experience to justify enumerative induction by having shown uniformity of nature,[9] Mill welcomed Comte's positivism, but thought laws susceptible to recall or revision, and withheld from Comte's Religion of Humanity. Comte was confident to lay laws as irrefutable foundation of other knowledge, and the churches, honoring eminent scientists, sought to focus public mindset on altruism—a term Comte coined—to apply science for humankind's social welfare via Comte's spearheaded science, sociology. During the 1830s and 1840s, while Comte and Mill were the leading philosophers of science, William Whewell found enumerative induction not nearly so simple, but, amid the dominance of inductivism, described "superinduction".[10] Whewell proposed recognition of "the peculiar import of the term Induction", as "there is some Conception superinduced upon the facts", that is, "the Invention of a new Conception in every inductive inference". Rarely spotted by Whewell's predecessors, such mental inventions rapidly evade notice.[10] Whewell explained, "Although we bind together facts by superinducing upon them a new Conception, this Conception, once introduced and applied, is looked upon as inseparably connected with the facts, and necessarily implied in them. Having once had the phenomena bound together in their minds in virtue of the Conception, men can no longer easily restore them back to detached and incoherent condition in which they were before they were thus combined".[10] These "superinduced" explanations may well be flawed, but their accuracy is suggested when they exhibit what Whewell termed consilience—that is, simultaneously predicting the inductive generalizations in multiple areas—a feat that, according to Whewell, can establish their truth. Perhaps to accommodate prevailing view of science as inductivist method, Whewell devoted several chapters to "methods of induction" and sometimes said "logic of induction"—and yet stressed it lacks rules and cannot be trained.[10] Originator of pragmatism, C S Peirce who, as did Gottlob Frege independently, in the 1870s performed vast investigations that clarified the basis of deductive inference as mathematical proof, recognized induction but continuously insisted on a third type of inference that Peirce variously termed abduction or retroduction or hypothesis or presumption.[11] Later philosophers gave Peirce's abduction, etc, the synonym inference to the best explanation (IBE).[12] Contemporary philosophy[edit] Bertrand Russell[edit] Having highlighted Hume's problem of induction, John Maynard Keynes posed logical probability as its answer—but then figured not quite.[13] Bertrand Russell found Keynes's Treatise on Probability the best examination of induction, and if read with Jean Nicod's Le Probleme logique de l'induction as well as R B Braithwaite's review of it in the October 1925 issue of Mind, to provide "most of what is known about induction", although the "subject is technical and difficult, involving a good deal of mathematics".[14] Two decades later, Russell proposed enumerative induction as an "independent logical principle".[15][16] Russell found, "Hume's skepticism rests entirely upon his rejection of the principle of induction. The principle of induction, as applied to causation, says that, if A has been found very often accompanied or followed by B, then it is probable that on the next occasion on which A is observed, it will be accompanied or followed by B. If the principle is to be adequate, a sufficient number of instances must make the probability not far short of certainty. If this principle, or any other from which it can be deduced, is true, then the casual inferences which Hume rejects are valid, not indeed as giving certainty, but as giving a sufficient probability for practical purposes. If this principle is not true, every attempt to arrive at general scientific laws from particular observations is fallacious, and Hume's skepticism is inescapable for an empiricist. The principle itself cannot, of course, without circularity, be inferred from observed uniformities, since it is required to justify any such inference. It must therefore be, or be deduced from, an independent principle not based on experience. To this extent, Hume has proved that pure empiricism is not a sufficient basis for science. But if this one principle is admitted, everything else can proceed in accordance with the theory that all our knowledge is based on experience. It must be granted that this is a serious departure from pure empiricism, and that those who are not empiricists may ask why, if one departure is allowed, others are forbidden. These, however, are not questions directly raised by Hume's arguments. What these arguments prove—and I do not think the proof can be controverted—is that the induction is an independent logical principle, incapable of being inferred either from experience or from other logical principles, and that without this principle, science is impossible".[16] Gilbert Harman[edit] In a 1965 paper, Gilbert Harman explained that enumerative induction is not an autonomous phenomenon, but is simply a masked consequence of inference to the best explanation (IBE).[12] IBE is otherwise synonym to C S Peirce's abduction.[12] Many philosophers of science espousing scientific realism have maintained that IBE is the way that scientists develop approximately true scientific theories about nature.[17] Criticism[edit] Main article: Problem of induction Inductive reasoning has been criticized by thinkers as far back as Sextus Empiricus.[18] The classic philosophical treatment of the problem of induction was given by the Scottish philosopher David Hume.[19] Although the use of inductive reasoning demonstrates considerable success, its application has been questionable. Recognizing this, Hume highlighted the fact that our mind draws uncertain conclusions from relatively limited experiences. In deduction, the truth value of the conclusion is based on the truth of the premise. In induction, however, the dependence on the premise is always uncertain. As an example, let's assume "all ravens are black." The fact that there are numerous black ravens supports the assumption. However, the assumption becomes inconsistent with the fact that there are white ravens. Therefore, the general rule of "all ravens are black" is inconsistent with the existence of the white raven. Hume further argued that it is impossible to justify inductive reasoning: specifically, that it cannot be justified deductively, so our only option is to justify it inductively. Since this is circular he concluded that our use of induction is unjustifiable with the help of Hume's Fork.[20] However, Hume then stated that even if induction were proved unreliable, we would still have to rely on it. So instead of a position of severe skepticism, Hume advocated a practical skepticism based on common sense, where the inevitability of induction is accepted.[21] Bertrand Russell illustrated his skepticism in a story about a turkey, fed every morning without fail, who following the laws of induction concludes this will continue, but then his throat is cut on Thanksgiving Day.[22] Karl Popper.[23] had declared in 1963, "Induction, i.e. inference based on many observations, is a myth. It is neither a psychological fact, nor a fact of ordinary life, nor one of scientific procedure".[24] Popper's 1972 book Objective Knowledge—whose first chapter is devoted to the problem of induction—opens, "I think I have solved a major philosophical problem: the problem of induction".[24] Within Popper's schema, enumerative induction is "a kind of optical illusion" cast by the steps of conjecture and refutation during the problem shift.[24] An imaginative leap, the tentative solution is improvised, lacking inductive rules to guide it.[24] The resulting, unrestricted generalization is deductive, an entailed consequence of all, included explanatory considerations.[24] Controversy continued, however, with Popper's putative solution not generally accepted.[25] By now, inductive inference has been shown to exist, but is found rarely, as in programs of machine learning in Artificial Intelligence (AI).[26] Popper's stance on induction is strictly falsified—enumerative induction exists—but is overwhelmingly absent from science.[26] Although much talked of nowadays by philosophers, abduction or IBE lacks rules of inference and the discussants provide nothing resembling such, as the process proceeds by humans' imaginations and perhaps creativity.[26] Biases[edit] Inductive reasoning is also known as hypothesis construction because any conclusions made are based on current knowledge and predictions.[citation needed] As with deductive arguments, biases can distort the proper application of inductive argument, thereby preventing the reasoner from forming the most logical conclusion based on the clues. Examples of these biases include the availability heuristic, confirmation bias, and the predictable-world bias The availability heuristic causes the reasoner to depend primarily upon information that is readily available to them. People have a tendency to rely on information that is easily accessible in the world around them. For example, in surveys, when people are asked to estimate the percentage of people who died from various causes, most respondents would choose the causes that have been most prevalent in the media such as terrorism, and murders, and airplane accidents rather than causes such as disease and traffic accidents, which have been technically "less accessible" to the individual since they are not emphasized as heavily in the world around them. The confirmation bias is based on the natural tendency to confirm rather than to deny a current hypothesis. Research has demonstrated that people are inclined to seek solutions to problems that are more consistent with known hypotheses rather than attempt to refute those hypotheses. Often, in experiments, subjects will ask questions that seek answers that fit established hypotheses, thus confirming these hypotheses. For example, if it is hypothesized that Sally is a sociable individual, subjects will naturally seek to confirm the premise by asking questions that would produce answers confirming that Sally is in fact a sociable individual. The predictable-world bias revolves around the inclination to perceive order where it has not been proved to exist, either at all or at a particular level of abstraction. Gambling, for example, is one of the most popular examples of predictable-world bias. Gamblers often begin to think that they see simple and obvious patterns in the outcomes and, therefore, believe that they are able to predict outcomes based upon what they have witnessed. In reality, however, the outcomes of these games are difficult to predict and highly complex in nature. However, in general, people tend to seek some type of simplistic order to explain or justify their beliefs and experiences, and it is often difficult for them to realise that their perceptions of order may be entirely different from the truth.[27] Types and examples[edit] The following are types of inductive argument. Notice that while similar, each has a different form. In contrast to the binary valid/invalid for deductive arguments, inductive arguments are rated in terms of strong or weak along a continuum. An inductive argument is strong in proportion to the probability that its conclusion is correct. We may call an inductive argument plausible, probable, reasonable, justified or strong, but never certain or necessary. Logic affords no bridge from the probable to the certain. The futility of attaining certainty through some critical mass of probability can be illustrated with a coin-toss exercise. Suppose someone shows me a coin and says the coin is either a fair one or two-headed. He flips it ten times, and ten times it comes up heads. At this point there is strong reason to believe it is two-headed. After all, the chance of ten heads in a row is .000976 – less than one in one thousand. Then, after 100 flips, still every toss has come up heads. Now there is “virtual” certainty that the coin is two-headed. Still, one can neither logically or empirically rule out that the next toss will produce tails. No matter how many times in a row it comes up heads this remains the case. If one programed a machine to flip a coin over and over continuously, at some point the result would be a string of 100 heads. In the fullness of time all combinations will appear. As for the slim prospect of getting ten out of ten heads from a fair coin - the outcome that made the coin appear biased - many may be surprised to learn that the chance of any combination of heads or tails is equally unlikely (e.g. H-H-T-T-H-T-H-H-H-T) – and yet it occurs in every trial of ten tosses. That means all results for ten tosses have the same probability as getting ten out of ten heads, which is .000976. If one records the heads-tails series, for whatever result, that exact series had a chance of .000976. The conclusion for a valid deductive argument is already contained in the premises since because its truth is strictly a matter of logical relations. It cannot say more than its premises. Inductive premises, on the other hand, draw their substance from fact and evidence, and the conclusion accordingly makes a factual claim or prediction. Its reliability varies proportionally with the evidence. Induction wants to reveal something new about the world. One could say that induction wants to say more than is contained in the premises. To better see the difference between inductive and deductive arguments, consider that it would not make sense to say, "All rectangles so far examined have four right angles, so the next one I see will have four right angles." This would treat logical relations as something factual and discoverable, and thus variable and uncertain. Likewise, speaking deductively we may permissibly say. "All unicorns can fly; I have a unicorn named Charlie; Charlie can fly." This deductive argument is valid because the logical relations hold; we are not interested in their factual soundness. A faulty inductive argument might take the form, "All Swans so far observed were white, therefore it is settled that all swans white." This argument is a case of induction posing as deduction, and fails for the reasons discussed above. Inductive reasoning is inherently uncertain. It only deals in degrees to which, given the premises, the conclusion is credible according to some theory of evidence. Examples include a many-valued logic, Dempster–Shafer theory, or probability theory with rules for inference such as Bayes' rule. Unlike deductive reasoning, it does not rely on universals holding over a closed domain of discourse to draw conclusions, so it can be applicable even in cases of epistemic uncertainty (technical issues with this may arise however; for example, the second axiom of probability is a closed-world assumption).[28] An example of an inductive argument: All biological life forms that we know of depend on liquid water to exist. Therefore, if we discover a new biological life form it will probably depend on liquid water to exist. This argument could have been made every time a new biological life form was found, and would have been correct every time; however, it is still possible that in the future a biological life form not requiring liquid water could be discovered. As a result, the argument may be stated less formally as: All biological life forms that we know of depend on liquid water to exist. All biological life probably depends on liquid water to exist. Generalization[edit] A generalization (more accurately, an inductive generalization) proceeds from a premise about a sample to a conclusion about the population. The proportion Q of the sample has attribute A. Therefore: The proportion Q of the population has attribute A. Example There are 20 balls—either black or white—in an urn. To estimate their respective numbers, you draw a sample of four balls and find that three are black and one is white. A good inductive generalization would be that there are 15 black and five white balls in the urn. How much the premises support the conclusion depends upon (a) the number in the sample group, (b) the number in the population, and (c) the degree to which the sample represents the population (which may be achieved by taking a random sample). The hasty generalization and the biased sample are generalization fallacies. Statistical and inductive generalization[edit] Of a sizeable random sample of voters surveyed 66% supports Measure Z. Therefore, approximately 66% of voters supports Measure Z. This is a Statistical [29], aka Sample Projection.[30] The measure is highly reliable within a well-defined margin of error provided the sample is large and random. It is readily quantifiable. Compare the preceding argument with the following. “Six of the ten people in my book club are Libertarians. About 60% of people are Libertarians.” The argument is weak because the sample is non-random and the sample size is very small So far, this year his son's Little League team has won 6 of ten games. By season’s end, they will have won about 60% of the games. This is Inductive generalization. This inference is less reliable that the Statistical Generalization, first, because the sample events are non-random, and because it is not reducible to mathematical expression. Statistically speaking, there is simply no way to know, measure and calculate as to the circumstances affecting performance that will obtain in the future. On a philosophical level, the argument relies on the presupposition that the operation of future events will mirror the past. In other words, it takes for granted a uniformity of nature, an unproven principle that cannot be derived from the empirical data itself. Arguments that tacitly presuppose this uniformity are sometimes called Humean after the philosopher who was first to subject them to philosophical scrutiny. [31] Statistical syllogism[edit] Main article: Statistical syllogism A statistical syllogism proceeds from a generalization to a conclusion about an individual. 90% of graduates from Excelsior Preparatory school go on to University. Bob is a graduate of Excelsior Preparatory school. Bob will go on to University. This is a Statistical Syllogism.[32] Even though one cannot be sure Bob will attend university we can be fully assured of the exact probability for this outcome (given no further information). Arguably the argument is too strong and might be accused of “cheating.” After all, the probability is given in the premise. Typically, Inductive reasoning seeks to formulate a probability. Two dicto simpliciter fallacies can occur in statistical syllogisms: "accident" and "converse accident". Simple induction[edit] Simple induction proceeds from a premise about a sample group to a conclusion about another individual. Proportion Q of the known instances of population P has attribute A. Individual I is another member of P. Therefore: There is a probability corresponding to Q that I has A. This is a combination of a generalization and a statistical syllogism, where the conclusion of the generalization is also the first premise of the statistical syllogism. Enumerative induction[edit] The basic form of inductive inference, simply induction, reasons from particular instances to all instances, and is thus an unrestricted generalization.[33] If one observes 100 swans, and all 100 were white, one might infer a universal categorical proposition of the form All swans are white. As this reasoning form's premises, even if true, do not entail the conclusion's truth, this is a form of inductive inference. The conclusion might be true, and might be thought probably true, yet it can be false. Questions regarding the justification and form of enumerative inductions have been central in philosophy of science, as enumerative induction has a pivotal role in the traditional model of the scientific method. All life forms so far discovered are composed of cells. All life forms are composed of cells. This is Enumerative Induction, aka Simple Induction or Simple Predictive Induction. It is a subcategory of Inductive Generalization. In everyday practice this is perhaps the most common form of induction. For the preceding argument, the conclusion is tempting but makes a prediction well in excess of the evidence. First, it assumes that life forms observed until now can tell us how future cases will be – an appeal to uniformity. Second, the concluding All is a very bold assertion. A single contrary instance foils the argument. And last, to quantify the level of probability in any mathematical form is problematic. [34] By what standard do we measure our earthly sample of known life against all (possible) life? For suppose we do discover some new organism - let’s say some microorganism floating in the Mesosphere, or better yet, on some asteroid - and it is cellular. Doesn't the addition of this corroborating evidence oblige us to raise our probability assessment for the subject proposition? It is generally deemed reasonable to answer this question, yes; and for a good many this “yes” is not only reasonable but incontrovertible. Very well; so then just how much should this new data change our probability assessment. Here, consensus melts away; and in its place arises a question about whether we can talk of probability coherently at all without numerical quantification. All life forms so far discovered have been composed of cells. The next life form discovered will be composed of cells. This is Enumerative Induction in its weak form. It truncates “all” to a mere single instance, and by making a far weaker claim considerably strengthens the probability of its conclusion. Otherwise, it has the same shortcomings as the strong form: its sample population is non-random, and quantification methods are elusive. Argument from analogy[edit] Main article: Argument from analogy The process of analogical inference involves noting the shared properties of two or more things, and from this basis inferring that they also share some further property:[35] P and Q are similar in respect to properties a, b, and c. Object P has been observed to have further property x. Therefore, Q probably has property x also. Analogical reasoning is very frequent in common sense, science, philosophy and the humanities, but sometimes it is accepted only as an auxiliary method. A refined approach is case-based reasoning.[36] Mineral A is an igneous rock often containing veins of quartz, and most commonly found in South America in areas of ancient volcanic activity. Additionally, mineral A is soft stone suitable for carving into jewelry. Mineral B is an igneous rock often containing veins of quartz, and most commonly found in South America in areas of ancient volcanic activity. Mineral B is probably a soft stone suitable for carving into jewelry. This is analogical induction, according to which things alike in certain ways are more prone to be alike in other ways. This form of induction was explored in detail my philosopher John Stewart Mill in his System of Logic, wherein he states, "There can be no doubt that every resemblance [not known to be irrelevant] affords some degree :of probability, beyond what would otherwise exist, in favor of the conclusion."[37] Analogical induction is a subcategory of inductive generalization because it assumes a pre-established uniformity governing events. Analogical induction requires an auxiliary examination of the relevancy of the characteristics cited as common to the pair. In the preceding example, if I add the premise that both stones were mentioned in the records of early Spanish explorers, this common attribute is extraneous to the stones and does not contribute to their probable affinity. A pitfall of analogy is that features can be cherry-picked: While objects may show striking similarities, two things juxtaposed may respectively possess other characteristics not identified in the analogy that are characteristics sharply dissimilar. Thus, analogy can mislead if not all relevant comparisons are made. Causal inference[edit] A causal inference draws a conclusion about a causal connection based on the conditions of the occurrence of an effect. Premises about the correlation of two things can indicate a causal relationship between them, but additional factors must be confirmed to establish the exact form of the causal relationship. Prediction[edit] A prediction draws a conclusion about a future individual from a past sample. Proportion Q of observed members of group G have had attribute A. Therefore: There is a probability corresponding to Q that other members of group G will have attribute A when next observed. Bayesian inference[edit] As a logic of induction rather than a theory of belief, Bayesian inference does not determine which beliefs are a priori rational, but rather determines how we should rationally change the beliefs we have when presented with evidence. We begin by committing to a prior probability for a hypothesis based on logic or previous experience, and when faced with evidence, we adjust the strength of our belief in that hypothesis in a precise manner using Bayesian logic. Inductive inference[edit] Around 1960, Ray Solomonoff founded the theory of universal inductive inference, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. This is a formal inductive framework that combines algorithmic information theory with the Bayesian framework. Universal inductive inference is based on solid philosophical foundations,[38] and can be considered as a mathematically formalized Occam's razor. Fundamental ingredients of the theory are the concepts of algorithmic probability and Kolmogorov complexity. See also[edit] Thinking portal Logic portal Abductive reasoning Algorithmic probability Analogy Bayesian probability Counterinduction Deductive reasoning Explanation Failure mode and effects analysis Falsifiability Grammar induction Inductive inference Inductive logic programming Inductive probability Inductive programming Inductive reasoning aptitude Inductivism Inquiry Kolmogorov complexity Lateral thinking Laurence Jonathan Cohen Logic Logical positivism Machine learning Mathematical induction Mill's Methods Minimum description length Minimum message length New riddle of induction Open world assumption Raven paradox Recursive Bayesian estimation Retroduction Solomonoff's theory of inductive inference Statistical inference Stephen Toulmin Marcus Hutter References[edit] ^ Copi, I. M.; Cohen, C.; Flage, D. E. (2006). Essentials of Logic (Second ed.). Upper Saddle River, NJ: Pearson Education. ISBN 978-0-13-238034-8. ^ "Deductive and Inductive Arguments", Internet Encyclopedia of Philosophy, Some dictionaries define "deduction" as reasoning from the general to specific and "induction" as reasoning from the specific to the general. While this usage is still sometimes found even in philosophical and mathematical contexts, for the most part, it is outdated. ^ John Vickers. The Problem of Induction. The Stanford Encyclopedia of Philosophy. ^ Herms, D. "Logical Basis of Hypothesis Testing in Scientific Research" (pdf). ^ "Stanford Encyclopedia of Philosophy : Kant's account of reason". ^ Chowdhry, K.R. (January 2, 2015). Fundamentals of Discrete Mathematical Structures (3rd ed.). PHI Learning Pvt. Ltd. p. 26. ISBN 9788120350748. Retrieved 1 December 2016. ^ Introduction to Logic. Harry J. Gensler, Rutledge, 2002. P. 268 ^ a b Stefano Gattei, Karl Popper's Philosophy of Science: Rationality without Foundations (New York: Routledge, 2009), ch. 2 "Science and philosophy", pp 28–30. ^ a b Wesley C Salmon, "The uniformity of Nature", Philosophy and Phenomenological Research, 1953 Sep;14(1):39–48, p 39. ^ a b c d Roberto Torretti, The Philosophy of Physics (Cambridge: Cambridge University Press, 1999), pp 216, 219–21. ^ Roberto Torretti, The Philosophy of Physics (Cambridge: Cambridge University Press, 1999), pp 226, 228–29. ^ a b c Ted Poston "Foundationalism", § b "Theories of proper inference", §§ iii "Liberal inductivism", Internet Encyclopedia of Philosophy, 10 Jun 2010 (last updated): "Strict inductivism is motivated by the thought that we have some kind of inferential knowledge of the world that cannot be accommodated by deductive inference from epistemically basic beliefs. A fairly recent debate has arisen over the merits of strict inductivism. Some philosophers have argued that there are other forms of nondeductive inference that do not fit the model of enumerative induction. C. S. Peirce describes a form of inference called 'abduction' or 'inference to the best explanation'. This form of inference appeals to explanatory considerations to justify belief. One infers, for example, that two students copied answers from a third because this is the best explanation of the available data—they each make the same mistakes and the two sat in view of the third. Alternatively, in a more theoretical context, one infers that there are very small unobservable particles because this is the best explanation of Brownian motion. Let us call 'liberal inductivism' any view that accepts the legitimacy of a form of inference to the best explanation that is distinct from enumerative induction. For a defense of liberal inductivism, see Gilbert Harman's classic (1965) paper. Harman defends a strong version of liberal inductivism according to which enumerative induction is just a disguised form of inference to the best explanation". ^ David Andrews, Keynes and the British Humanist Tradition: The Moral Purpose of the Market (New York: Routledge, 2010), pp 63–65. ^ Bertrand Russell, The Basic Writings of Bertrand Russell (New York: Routledge, 2009), "The validity of inference"], pp 157–64, quote on p 159. ^ Gregory Landini, Russell (New York: Routledge, 2011), p 230. ^ a b Bertrand Russell, A History of Western Philosophy (London: George Allen and Unwin, 1945 / New York: Simon and Schuster, 1945), pp 673-74. ^ Stathis Psillos, "On Van Fraassen's critique of abductive reasoning", Philosophical Quarterly, 1996 Jan;46(182):31–47, p 31. ^ Sextus Empiricus, Outlines of Pyrrhonism. Trans. R.G. Bury, Harvard University Press, Cambridge, Massachusetts, 1933, p. 283. ^ David Hume (1910) [1748]. An Enquiry concerning Human Understanding. P.F. Collier & Son. ISBN 0-19-825060-6. Archived from the original on 31 December 2007. Retrieved 27 December 2007. ^ Vickers, John. "The Problem of Induction" (Section 2). Stanford Encyclopedia of Philosophy. 21 June 2010 ^ Vickers, John. "The Problem of Induction" (Section 2.1). Stanford Encyclopedia of Philosophy. 21 June 2010. ^ The story by Russell is found in Alan Chalmers, What is this thing Called Science, Open University Press, Milton Keynes, 1982, p. 14 ^ Popper, Karl R.; Miller, David W. (1983). "A proof of the impossibility of inductive probability". Nature. 302 (5910): 687–688. Bibcode:1983Natur.302..687P. doi:10.1038/302687a0. ^ a b c d e Donald Gillies, "Problem-solving and the problem of induction", in Rethinking Popper (Dordrecht: Springer, 2009), Zuzana Parusniková & Robert S Cohen, eds, pp 103–05. ^ Ch 5 "The controversy around inductive logic" in Richard Mattessich, ed, Instrumental Reasoning and Systems Methodology: An Epistemology of the Applied and Social Sciences (Dordrecht: D. Reidel Publishing, 1978), pp 141–43. ^ a b c Donald Gillies, "Problem-solving and the problem of induction", in Rethinking Popper (Dordrecht: Springer, 2009), Zuzana Parusniková & Robert S Cohen, eds, p 111: "I argued earlier that there are some exceptions to Popper's claim that rules of inductive inference do not exist. However, these exceptions are relatively rare. They occur, for example, in the machine learning programs of AI. For the vast bulk of human science both past and present, rules of inductive inference do not exist. For such science, Popper's model of conjectures which are freely invented and then tested out seems to me more accurate than any model based on inductive inferences. Admittedly, there is talk nowadays in the context of science carried out by humans of 'inference to the best explanation' or 'abductive inference', but such so-called inferences are not at all inferences based on precisely formulated rules like the deductive rules of inference. Those who talk of 'inference to the best explanation' or 'abductive inference', for example, never formulate any precise rules according to which these so-called inferences take place. In reality, the 'inferences' which they describe in their examples involve conjectures thought up by human ingenuity and creativity, and by no means inferred in any mechanical fashion, or according to precisely specified rules". ^ Gray, Peter (2011). Psychology (Sixth ed.). New York: Worth. ISBN 978-1-4292-1947-1. ^ Kosko, Bart (1990). "Fuzziness vs. Probability". International Journal of General Systems. 17 (1): 211–240. doi:10.1080/03081079008935108. ^ Schaum’s Outlines, Logic, Second Edition. John Nolt, Dennis Rohatyn, Archille Varzi.McGraw-Hill, 1998. P. 223 ^ Schaum’s Outlines, Logic, p. 230 ^ Introduction to Logic. Gensler p. 280 ^ Introduction to Logic. Harry J. Gensler, Rutledge, 2002. P. 268 ^ Churchill, Robert Paul (1990). Logic: An Introduction (2nd ed.). New York: St. Martin's Press. p. 355. ISBN 0-312-02353-7. OCLC 21216829. In a typical enumerative induction, the premises list the individuals observed to have a common property, and the conclusion claims that all individuals of the same population have that property. ^ Schaum’s Outlines, Logic, p. 243-235 ^ Baronett, Stan (2008). Logic. Upper Saddle River, NJ: Pearson Prentice Hall. pp. 321–325. ^ For more information on inferences by analogy, see Juthe, 2005. ^ A System of Logic. Mill 1843/1930. p 333 ^ Rathmanner, Samuel; Hutter, Marcus (2011). "A Philosophical Treatise of Universal Induction". Entropy. 13 (6): 1076–1136. arXiv:1105.5721. Bibcode:2011Entrp..13.1076R. doi:10.3390/e13061076. Further reading[edit] Cushan, Anna-Marie (1983/2014). Investigation into Facts and Values: Groundwork for a theory of moral conflict resolution. [Thesis, Melbourne University], Ondwelle Publications (online): Melbourne. [1] Herms, D. "Logical Basis of Hypothesis Testing in Scientific Research" (PDF). Kemerling, G. (27 October 2001). "Causal Reasoning". Holland, J. H.; Holyoak, K. J.; Nisbett, R. E.; Thagard, P. R. (1989). Induction: Processes of Inference, Learning, and Discovery. Cambridge, MA: MIT Press. ISBN 0-262-58096-9. Holyoak, K.; Morrison, R. (2005). The Cambridge Handbook of Thinking and Reasoning. New York: Cambridge University Press. ISBN 978-0-521-82417-0. External links[edit] Wikiquote has quotations related to: Inductive reasoning Look up inductive reasoning in Wiktionary, the free dictionary. Wikisource has the text of a 1920 Encyclopedia Americana article about Inductive reasoning. "Confirmation and Induction". Internet Encyclopedia of Philosophy. Zalta, Edward N. (ed.). "Inductive Logic". Stanford Encyclopedia of Philosophy. Inductive reasoning at PhilPapers Inductive reasoning at the Indiana Philosophy Ontology Project Four Varieties of Inductive Argument from the Department of Philosophy, University of North Carolina at Greensboro. "Properties of Inductive Reasoning" (PDF). (166 KiB), a psychological review by Evan Heit of the University of California, Merced. The Mind, Limber An article which employs the film The Big Lebowski to explain the value of inductive reasoning. The Pragmatic Problem of Induction, by Thomas Bullemore Links to related articles v t e Learning Simple non-associative learning Habituation Sensitization Associative learning Operant conditioning Classical conditioning Imprinting Observational learning Insight learning Deductive reasoning Inductive reasoning Abductive reasoning v t e Logic Outline History Fields Argumentation theory Axiology Critical thinking Logic in computer science Mathematical logic Metalogic Metamathematics Non-classical logic Philosophical logic Philosophy of logic Set theory Foundations Abduction Analytic and synthetic propositions Antinomy A priori and a posteriori Deduction Definition Description Induction Inference Logical form Logical consequence Logical truth Name Necessity and sufficiency Meaning Paradox Possible world Presupposition Probability Reason Reference Semantics Statement Strict implication Substitution Syntax Truth Validity Lists topics Mathematical logic Boolean algebra Set theory other Logicians Rules of inference Paradoxes Fallacies Logic symbols Portal Category WikiProject (talk) changes v t e Philosophy of science Concepts Analysis Analytic–synthetic distinction A priori and a posteriori Causality Commensurability Consilience Construct Creative synthesis Demarcation problem Empirical evidence Explanatory power Fact Falsifiability Feminist method Ignoramus et ignorabimus Inductive reasoning Intertheoretic reduction Inquiry Nature Objectivity Observation Paradigm Problem of induction Scientific law Scientific method Scientific revolution Scientific theory Testability Theory choice Theory-ladenness Underdetermination Unity of science Metatheory of science Coherentism Confirmation holism Constructive empiricism Constructive realism Constructivist epistemology Contextualism Conventionalism Deductive-nomological model Hypothetico-deductive model Inductionism Epistemological anarchism Evolutionism Fallibilism Foundationalism Instrumentalism Pragmatism Model-dependent realism Naturalism Physicalism Positivism / Reductionism / Determinism Rationalism / Empiricism Received view / Semantic view of theories Scientific realism / Anti-realism Scientific essentialism Scientific formalism Scientific skepticism Scientism Structuralism Uniformitarianism Vitalism Philosophy of Physics thermal and statistical Motion Chemistry Biology Environment Geography Social science Technology Engineering Artificial intelligence Computer science Information Mind Psychiatry Psychology Perception Space and time Related topics Alchemy Criticism of science Epistemology Faith and rationality History and philosophy of science History of science History of evolutionary thought Logic Metaphysics Pseudoscience Relationship between religion and science Rhetoric of science Sociology of scientific knowledge Sociology of scientific ignorance Philosophers of science by era Ancient Plato Aristotle Stoicism Epicureans Medieval Averroes Avicenna Roger Bacon William of Ockham Hugh of Saint Victor Dominicus Gundissalinus Robert Kilwardby Early modern Francis Bacon Thomas Hobbes René Descartes Galileo Galilei Pierre Gassendi Isaac Newton David Hume Late modern Immanuel Kant Friedrich Schelling William Whewell Auguste Comte John Stuart Mill Herbert Spencer Wilhelm Wundt Charles Sanders Peirce Wilhelm Windelband Henri Poincaré Pierre Duhem Rudolf Steiner Karl Pearson Contemporary Alfred North Whitehead Bertrand Russell Albert Einstein Otto Neurath C. D. Broad Michael Polanyi Hans Reichenbach Rudolf Carnap Karl Popper Carl Gustav Hempel W. V. O. Quine Thomas Kuhn Imre Lakatos Paul Feyerabend Jürgen Habermas Ian Hacking Bas van Fraassen Larry Laudan Daniel Dennett Portal Category v t e Positivism Perspectives Antihumanism Empiricism Rationalism Scientism Declinations Legal positivism Logical positivism / analytic philosophy Positivist school Postpositivism Sociological positivism Machian positivism (empirio-criticism) Rankean historical positivism Polish positivism Russian Machism Principal concepts Consilience Demarcation Evidence Induction Justificationism Pseudoscience Critique of metaphysics Unity of science Verificationism Antitheses Antipositivism Confirmation holism Critical theory Falsifiability Geisteswissenschaft Hermeneutics Historicism Historism Human science Humanities Problem of induction Reflectivism Related paradigm shifts in the history of science Non-Euclidean geometry (1830s) Uncertainty principle (1927) Related topics Behavioralism Post-behavioralism Critical rationalism Criticism of science Epistemology anarchism idealism nihilism pluralism realism Holism Instrumentalism Modernism Naturalism in literature Nomothetic–idiographic distinction Objectivity in science Operationalism Phenomenalism Philosophy of science Deductive-nomological model Ramsey sentence Sense-data theory Qualitative research Relationship between religion and science Sociology Social science Philosophy Structural functionalism Structuralism Structuration theory Positivist-related debate Method Methodenstreit (1890s) Werturteilsstreit (1909–1959) Positivismusstreit (1960s) Fourth Great Debate in international relations (1980s) Science wars (1990s) Contributions The Course in Positive Philosophy (1830) A General View of Positivism (1848) Critical History of Philosophy (1869) Idealism and Positivism (1879–1884) The Analysis of Sensations (1886) The Logic of Modern Physics (1927) Language, Truth, and Logic (1936) The Two Cultures (1959) The Universe in a Nutshell (2001) Proponents Richard Avenarius A. J. Ayer Alexander Bogdanov Auguste Comte Eugen Dühring Émile Durkheim Ernst Laas Ernst Mach Berlin Circle Vienna Circle Criticism Materialism and Empirio-criticism (1909) History and Class Consciousness (1923) The Logic of Scientific Discovery (1934) The Poverty of Historicism (1936) World Hypotheses (1942) Two Dogmas of Empiricism (1951) Truth and Method (1960) The Structure of Scientific Revolutions (1962) Conjectures and Refutations (1963) One-Dimensional Man (1964) Knowledge and Human Interests (1968) The Poverty of Theory (1978) The Scientific Image (1980) The Rhetoric of Economics (1986) Critics Theodor W. Adorno Gaston Bachelard Mario Bunge Wilhelm Dilthey Paul Feyerabend Hans-Georg Gadamer Thomas Kuhn György Lukács Karl Popper Willard Van Orman Quine Max Weber Concepts in contention Knowledge Objectivity Phronesis Truth Verstehen Category v t e Statistics Outline Index Descriptive statistics Continuous data Center Mean arithmetic geometric harmonic Median Mode Dispersion Variance Standard deviation Coefficient of variation Percentile Range Interquartile range Shape Central limit theorem Moments Skewness Kurtosis L-moments Count data Index of dispersion Summary tables Grouped data Frequency distribution Contingency table Dependence Pearson product-moment correlation Rank correlation Spearman's rho Kendall's tau Partial correlation Scatter plot Graphics Bar chart Biplot Box plot Control chart Correlogram Fan chart Forest plot Histogram Pie chart Q–Q plot Run chart Scatter plot Stem-and-leaf display Radar chart Data collection Study design Population Statistic Effect size Statistical power Sample size determination Missing data Survey methodology Sampling stratified cluster Standard error Opinion poll Questionnaire Controlled experiments Design control optimal Controlled trial Randomized Random assignment Replication Blocking Interaction Factorial experiment Uncontrolled studies Observational study Natural experiment Quasi-experiment Statistical inference Statistical theory Population Statistic Probability distribution Sampling distribution Order statistic Empirical distribution Density estimation Statistical model Lp space Parameter location scale shape Parametric family Likelihood (monotone) Location–scale family Exponential family Completeness Sufficiency Statistical functional Bootstrap U V Optimal decision loss function Efficiency Statistical distance divergence Asymptotics Robustness Frequentist inference Point estimation Estimating equations Maximum likelihood Method of moments M-estimator Minimum distance Unbiased estimators Mean-unbiased minimum-variance Rao–Blackwellization Lehmann–Scheffé theorem Median unbiased Plug-in Interval estimation Confidence interval Pivot Likelihood interval Prediction interval Tolerance interval Resampling Bootstrap Jackknife Testing hypotheses 1- & 2-tails Power Uniformly most powerful test Permutation test Randomization test Multiple comparisons Parametric tests Likelihood-ratio Wald Score Specific tests Z-test (normal) Student's t-test F-test Goodness of fit Chi-squared G-test Kolmogorov–Smirnov Anderson–Darling Lilliefors Jarque–Bera Normality (Shapiro–Wilk) Likelihood-ratio test Model selection Cross validation AIC BIC Rank statistics Sign Sample median Signed rank (Wilcoxon) Hodges–Lehmann estimator Rank sum (Mann–Whitney) Nonparametric anova 1-way (Kruskal–Wallis) 2-way (Friedman) Ordered alternative (Jonckheere–Terpstra) Bayesian inference Bayesian probability prior posterior Credible interval Bayes factor Bayesian estimator Maximum posterior estimator Correlation Regression analysis Correlation Pearson product-moment Partial correlation Confounding variable Coefficient of determination Regression analysis Errors and residuals Regression model validation Mixed effects models Simultaneous equations models Multivariate adaptive regression splines (MARS) Linear regression Simple linear regression Ordinary least squares General linear model Bayesian regression Non-standard predictors Nonlinear regression Nonparametric Semiparametric Isotonic Robust Heteroscedasticity Homoscedasticity Generalized linear model Exponential families Logistic (Bernoulli) / Binomial / Poisson regressions Partition of variance Analysis of variance (ANOVA, anova) Analysis of covariance Multivariate ANOVA Degrees of freedom Categorical / Multivariate / Time-series / Survival analysis Categorical Cohen's kappa Contingency table Graphical model Log-linear model McNemar's test Multivariate Regression Manova Principal components Canonical correlation Discriminant analysis Cluster analysis Classification Structural equation model Factor analysis Multivariate distributions Elliptical distributions Normal Time-series General Decomposition Trend Stationarity Seasonal adjustment Exponential smoothing Cointegration Structural break Granger causality Specific tests Dickey–Fuller Johansen Q-statistic (Ljung–Box) Durbin–Watson Breusch–Godfrey Time domain Autocorrelation (ACF) partial (PACF) Cross-correlation (XCF) ARMA model ARIMA model (Box–Jenkins) Autoregressive conditional heteroskedasticity (ARCH) Vector autoregression (VAR) Frequency domain Spectral density estimation Fourier analysis Wavelet Whittle likelihood Survival Survival function Kaplan–Meier estimator (product limit) Proportional hazards models Accelerated failure time (AFT) model First hitting time Hazard function Nelson–Aalen estimator Test Log-rank test Applications Biostatistics Bioinformatics Clinical trials / studies Epidemiology Medical statistics Engineering statistics Chemometrics Methods engineering Probabilistic design Process / quality control Reliability System identification Social statistics Actuarial science Census Crime statistics Demography Econometrics National accounts Official statistics Population statistics Psychometrics Spatial statistics Cartography Environmental statistics Geographic information system Geostatistics Kriging Category Portal Commons WikiProject Retrieved from "https://en.wikipedia.org/w/index.php?title=Inductive_reasoning&oldid=871771040" Categories: Arguments Inductive reasoning Epistemology Problem solving skills Reasoning Philosophy of statistics Causal inference Epistemology of science Hidden categories: Wikipedia introduction cleanup from September 2018 All pages needing cleanup Articles covered by WikiProject Wikify from September 2018 All articles covered by WikiProject Wikify Wikipedia articles needing copy edit from October 2018 All articles needing copy edit Use dmy dates from July 2013 Wikipedia articles needing clarification from April 2018 All articles with unsourced statements Articles with unsourced statements from March 2012 Articles containing German-language text Navigation menu Personal tools Not logged in Talk Contributions Create account Log in Namespaces Article Talk Variants Views Read Edit View history More Search Navigation Main page Contents Featured content Current events Random article Donate to Wikipedia Wikipedia store Interaction Help About Wikipedia Community portal Recent changes Contact page Tools What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page Print/export Create a book Download as PDF Printable version In other projects Wikimedia Commons Wikiquote Languages العربية Azərbaycanca Български Bosanski Català Čeština Dansk Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Hrvatski Bahasa Indonesia Íslenska Italiano עברית Latviešu Lietuvių Magyar Монгол Nederlands 日本語 Norsk Oʻzbekcha/ўзбекча Polski Português Română Русский Shqip Simple English Slovenčina Slovenščina Српски / srpski Srpskohrvatski / српскохрватски Suomi Svenska தமிழ் ไทย Тоҷикӣ Türkçe Türkmençe Українська اردو Tiếng Việt 粵語 中文 Edit links This page was last edited on 3 December 2018, at 09:19 (UTC). Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Developers Cookie statement Mobile view
